[{"authors":["admin"],"categories":null,"content":"I'm currently a postdoctoral teaching and learning fellow with the Master of Data Science program at the University of British Columbia, Vancouver, Canada. I have experience in research, teaching, and engineering consulting. I have a background in the Earth sciences and civil engineering, but now teach and practice data science. In particular, I'm interested in the practical application of data science to solving real-world problems üåè. ","date":1589068800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1589068800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.tomasbeuzen.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I'm currently a postdoctoral teaching and learning fellow with the Master of Data Science program at the University of British Columbia, Vancouver, Canada. I have experience in research, teaching, and engineering consulting. I have a background in the Earth sciences and civil engineering, but now teach and practice data science. In particular, I'm interested in the practical application of data science to solving real-world problems üåè. ","tags":null,"title":"Tomas Beuzen","type":"authors"},{"authors":["Tomas Beuzen"],"categories":[],"content":" Introduction Recently I had to do some clustering of data that contained both continuous and categorical features. Standard clustering algorithms like k-means and DBSCAN don\u0026rsquo;t work with categorical data. After doing some research, I found that there wasn\u0026rsquo;t really a standard approach to the problem. So, I came up with a few different approaches, the practical implementations of which I\u0026rsquo;m documenting here and which I plan to come back to investigate further at some point:\n Cluster using e.g., k-means or DBSCAN, based on only the continuous features; Numerically encode the categorical data before clustering with e.g., k-means or DBSCAN; Use k-prototypes to directly cluster the mixed data; Use FAMD (factor analysis of mixed data) to reduce the mixed data to a set of derived continuous features which can then be clustered.  I\u0026rsquo;ll describe each approach in a little more detail below, but first, if you plan to follow along, you\u0026rsquo;ll need to install the prince and kmodes Python packages:\npip install prince pip install kmodes  The post comes with a Jupyter notebook which you can find here on Github. Let\u0026rsquo;s get to our Python imports:\nimport numpy as np import pandas as pd from prince import FAMD from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from kmodes.kprototypes import KPrototypes from sklearn.preprocessing import StandardScaler random_state = 1234 pd.options.plotting.backend = \u0026quot;plotly\u0026quot;  I also defined a custom plotting function to use in this post which leverages Pandas brand new plotly plotting backend:\ndef plot_cluster(X, y, title=\u0026quot;Cluster plot\u0026quot;): fig = X.plot.scatter(x='X1', y='X2', color=y) fig.update_layout(autosize=False, width=500, height=500, coloraxis = dict(showscale=False, colorscale='Portland'), font=dict(size=18), title=dict(text=title, x=0.5, y=0.95, xanchor='center')) fig.update_traces(marker=dict(size=15)) return fig  Finally, I\u0026rsquo;ll create some synthetic data to demonstrate the clustering methods discussed in this post. The data will have 50 observations, 3 features and 3 clusters. I standardise the numerical data with sklearn\u0026rsquo;s StandardScaler() for clustering purposes (to make sure all features are on the same scale), and pretty arbitrarily convert one of the features to a categorical of \u0026ldquo;LOW\u0026rdquo; and \u0026ldquo;HIGH\u0026rdquo; values to demonstrate different approaches to clustering mixed data.\nX, y = make_blobs(n_samples=50, centers=3, n_features=3, random_state=random_state) X = pd.DataFrame(X, columns=['X1', 'X2', 'X3']) X['X3'] = np.where(X['X3'] \u0026lt; 0, 'LOW', 'HIGH') con_feats = ['X1', 'X2'] cat_feats = ['X3'] scale = StandardScaler() X[con_feats] = scale.fit_transform(X[con_feats]) X.head()      X1 X2 X3     0 -0.495194 0.963114 HIGH   1 -0.548021 -1.762852 LOW   2 1.101047 0.935499 LOW   3 -0.694720 -1.779252 LOW   4 1.261093 0.964404 LOW    Let\u0026rsquo;s plot our synthetic data (using our two continuous features as the x and y axes). There are 3 quite distinct blobs shown in blue, red, and yellow. However, there is a bit of mixture evident in the blue and red blobs and it will be interesting to explore how our different clustering approaches can capture this.\nplot_cluster(X, y, title=\u0026quot;True Data\u0026quot;)  Figure 1: Scatter plot of synthetic data coloured by true labels.\n1. Cluster based on continuous data only The first question I asked myself when dealing with my mixed data was \u0026ldquo;Do I really need the information contained in the categorical features to extract patterns in my dataset?\u0026rdquo;. It could be that the continuous features available to you in your mixed data are adequate for grouping the data into representative clusters. So the first thing we\u0026rsquo;ll try here is to simply ignore our single categorical feature (which standard algorithms like k-means and DBSCAN don\u0026rsquo;t like), and only cluster based on our continuous features.\nmodel = KMeans(n_clusters=3, random_state=random_state).fit(X[con_feats]) pred = model.labels_ plot_cluster(X, pred, title=\u0026quot;Continuous Only\u0026quot;)  Figure 2: Clustering based on only the continuous features.\nThe results are not too bad, we pick up the 3 main clusters, but do not identify that mixed data around (X1=-1, X2=0) evident in the true data (Figure 1).\n2. Encode the cateogircal data before clustering Next we\u0026rsquo;ll try encoding the categorical data using one hot encoding so that we can include it in k-means clustering (note that you may also want to try scaling the data after OHE but I didn\u0026rsquo;t do that here for succinctness).\nmodel = KMeans(n_clusters=3, random_state=random_state).fit(pd.get_dummies(X)) pred = model.labels_ plot_cluster(X, pred, title=\u0026quot;Encoded Categorical Data\u0026quot;)  Figure 3: Clustering after one hot encoding the categorical feature.\nThe results are better than before, we get our 3 blobs, plus we identify some of that mixed data around (X1=-1, X2=0).\n3. Use the k-prototypes algorithm The k-prototypes algorithm can work directly with the categorical data, without the need for encoding. I defer to the k-prototypes documentation and the original paper by Huang (1997) for an explanation of how the algorithm works.\npred = KPrototypes(n_clusters=3).fit_predict(X, categorical=[2]) plot_cluster(X, pred.astype(float), title=\u0026quot;k-prototypes\u0026quot;)  Figure 4: Clustering using k-prototypes.\nThe results are similar to the above, we get our 3 blobs, plus we identify some of that mixed data around (X1=-1, X2=0).\n4. Use FAMD to create continuous features for clustering Our final approach is to use FAMD (factor analysis for mixed data) to convert our mixed continuous and categorical data into derived continuous components (I chose 3 components here). I defer to the Prince documentation for an explanation of how the FAMD algorithm works.\nHere is an example of the 3 derived components for the first 5 observations in our synthetic dataset.\nfamd = FAMD(n_components=3).fit(X) famd.row_coordinates(X).head()      Component 1 Component 2 Component 3     0 -0.134396 7.103708 -0.530751   1 7.067240 0.113685 1.561657   2 7.090335 0.082113 -1.083236   3 7.060651 0.122537 1.649692   4 7.097300 0.072753 -1.186533    model = KMeans(n_clusters=3, random_state=random_state).fit(famd.row_coordinates(X)) pred = model.labels_ plot_cluster(X, pred, title=\u0026quot;FAMD + Clustering\u0026quot;)  Figure 5: Clustering using FAMD.\nThe results are interesting here, we do get our 3 blobs but the bottom left blob is not very uniform. However, we perfectly identify the mixed labels around (X1=-1, X2=0), which no previous approach has been able to do.\nSummary In this post I documented a few approaches for clustering mixed continuous and categorical data. As always with data science, there is no one approach suited to all problems - in my opinion, clustering in particular is as much an art as a science. But, for the specific real-world application I was working on, I ended up going with approach number 4 (FAMD + clustering), because it yielded the best results for my dataset (which was significantly more complex than the one in this post, with ~400 mixed categorical and continuous features).\n","date":1589068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589068800,"objectID":"42ae8d9cae71a42d60dfbdd69f32fa07","permalink":"https://www.tomasbeuzen.com/post/clustering-mixed-data/","publishdate":"2020-05-10T00:00:00Z","relpermalink":"/post/clustering-mixed-data/","section":"post","summary":"A short discussion of methods for clustering mixed datasets of categorical and continuous data.","tags":[],"title":"Unsupervised clustering with mixed categorical and continuous data","type":"post"},{"authors":["Tomas Beuzen"],"categories":[],"content":" Introduction I recently wrote up some tutorials on my GitHub to help data scientists deploy machine learning models. The aim of the tutorials is to provide a simple guide to deploying machine learning (ML) models for data scientists familiar with machine learning in a local environment, but interested in learning how to deploy their models. Deployment refers to the act of making your ML model available in a production environment, where it can be accessed and utilised by other software.\nPerhaps surprisingly, deployment is a process that is quite unfamiliar to many data scientists - in large part due to the need for some level of familiarity with software engineering. Fortunately, there are many tools avaialble to help us data scientists with deploying our models. The tutorials focus on currently and commonly used tools for ML deployment and are overwhelmingly practical, aiming to provide a useful overview of these tools and a foundation for using and expanding upon them in future. Here is a current list of tutorials, click a link to get started (to follow these tutorials, I recommend cloning the GitHub repository to your local machine):\n Building and deploying a machine learning model with Amazon Sagemaker Deploying a machine learning model with Flask and Heroku  ","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"46e13ab41328c7accbe23c135202a2de","permalink":"https://www.tomasbeuzen.com/post/deploy-ml-models/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/post/deploy-ml-models/","section":"post","summary":"Simple tutorials of how to build and deploy machine learning models with commonly used tools.","tags":[],"title":"Deploying machine learning models with Amazon SageMaker or Flask \u0026 Heroku","type":"post"},{"authors":["Tomas Beuzen"],"categories":[],"content":" Introduction Some of the key steps in a machine learning workflow are:\n feature preprocessing (encoding categorical features, scaling numeric features, transforming text data, etc.); feature selection (choosing which features to include in the model); model selection (choosing which machine learning estimator to use); and, hyperparameter tuning (determining the optimum hyperparameter values to use for each estimator).  It can be difficult to perform these tasks in an accurate, efficient and reproducible manner. In particular, it is important to ensure that, during cross-validation, feature preprocessing and feature selection are based only on the training portion of data, preventing leakage from the validation set which could bias our results. In this short, practical post I\u0026rsquo;ll demonstrate how to use scikit-learn to simultaneously perform the above steps. While the example given is somewhat contrived, the syntax and workflow are what is important here and can be applied to any machine learning workflow.\nStep 1: Import dependencies from sklearn.pipeline import Pipeline from sklearn.datasets import make_classification from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import SelectKBest, mutual_info_classif  Step 2: Import data We will create a synthetic binary classification dataset for this demonstration using the scikit-learn function make_classification.\nX, y = make_classification(n_samples=1000, n_features=30, n_informative=5, n_redundant=5, n_classes=2, random_state=123)  Step 3: Create pipeline framework Using our synthetic dataset, we are going to set up a pipeline object that will:\n Standardize the data using StandardScaler; Select the k best features from the data using SelectKBest and the mutual information metric (where k is a hyperparameter that we will tune during the fitting process); and, Use an estimator to model the data, here we will be trying a LogisticRegression, RandomForestClassifier, and KNeighborsClassifier.  The syntax for creating this pipeline is shown below. To instantiate the Pipeline object I\u0026rsquo;ve used a k value in SelectKBest of 5 and I\u0026rsquo;ve input LogisticRegression as the estimator, but these are simply placeholders for now and they will be varied during the fitting stage.\npipe = Pipeline([('scaler', StandardScaler()), ('selector', SelectKBest(mutual_info_classif, k=5)), ('classifier', LogisticRegression())])  Step 4: Create search space The next step is to define the space of hyperparameters and estimators we want to search through. We do this in the form of a dictionary and we use double underscore notation (__) to refer to the hyperparameters of different steps in our pipeline. We will be trying out different values of k for the feature selector SelectKBest, as well as different hyperparameter values for each of our three estimators as shown below.\nsearch_space = [{'selector__k': [5, 10, 20, 30]}, {'classifier': [LogisticRegression(solver='lbfgs')], 'classifier__C': [0.01, 0.1, 1.0]}, {'classifier': [RandomForestClassifier(n_estimators=100)], 'classifier__max_depth': [5, 10, None]}, {'classifier': [KNeighborsClassifier()], 'classifier__n_neighbors': [3, 7, 11], 'classifier__weights': ['uniform', 'distance']}]  Step 5: Run the GridSearch This is where the magic happens. We will now pass our pipeline into GridSearchCV to test our search space (of feature preprocessing, feature selection, model selection, and hyperparameter tuning combinations) using 10-fold cross-validation.\nclf = GridSearchCV(pipe, search_space, cv=10, verbose=0) clf = clf.fit(X, y)  Step 6: Get the results We can access the best result of our search using the best_estimator_ attribute. For this particular case, the KNeighborsClassifier did the best, using n_neighbors=3 and weights='distance', along with the k=5 best features chosen by SelectKBest. This combination had a 10-fold cross-validation accuracy of 0.958.\nclf.best_estimator_ Pipeline(memory=None, steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('selector', SelectKBest(k=5, score_func=\u0026lt;function mutual_info_classif at 0x1a1f91db00\u0026gt;)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=3, p=2, weights='distance'))], verbose=False) clf.best_score_ 0.958  ","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"5178b27001514a7320a191d620398782","permalink":"https://www.tomasbeuzen.com/post/scikit-learn-gridsearch-pipelines/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/post/scikit-learn-gridsearch-pipelines/","section":"post","summary":"How to easily perform simultaneous feature preprocessing, feature selection, model selection, and hyperparameter tuning in just a few lines of code using Python and scikit-learn.","tags":[],"title":"Simultaneous feature preprocessing, feature selection, model selection, and hyperparameter tuning in scikit-learn with Pipeline and GridSearchCV","type":"post"},{"authors":["Tomas Beuzen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"ea346015f1be4352daae219ca2f0565a","permalink":"https://www.tomasbeuzen.com/publication/journal-article-jgr/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/journal-article-jgr/","section":"publication","summary":"The erosion impact of large coastal storm events typically occurs across broad (100s of km) sections of coastline and may include significant variability both alongshore and vertically between the berm and dunes. Identifying controls of variability in storm erosion is critical to understanding the response of coastlines to present and changing storminess. This contribution analyses immediate pre‚Äê and post‚Äêstorm Lidar data of over 1700 cross‚Äêshore profile transects, determined at every 100 m alongshore and spanning 400km of the southeast Australian coastline. This unique dataset allowed for a data‚Äêdriven Bayesian network analysis of the key relationships between the measured storm erosion response and a range of variables describing the antecedent morphology and hydrodynamic forcing at the coastline. It was found that while erosion of the dune and berm was observed to increase with increased exposure of the local profile to incident storm waves, additional erosion controls were found to be different for these two different sections of the beach. Erosion of the berm was specifically linked to the pre‚Äêstorm berm volume, with more accreted berms experiencing a greater proportion of erosion of the overall berm, regardless of variability in forcing conditions. In contrast, dune erosion was equally controlled by the exceedance of wave runup above the antecedent dune toe elevation and the width of the beach immediately fronting the dune, with wider beaches resulting in reduced dune erosion. The results of this large, data‚Äêdriven analysis provide important affirmation and insights into the primary controls of berm and dune storm erosion.","tags":["Bayesian network","data","beach","dune","storm","erosion"],"title":"Controls of variability in berm and dune storm erosion","type":"publication"},{"authors":["Tomas Beuzen"],"categories":[],"content":" Introduction I primarily use Git and GitHub for my open-source work. However, if your anything like me, using these tools sometimes feels like a bit of a black box, nicely summarised by this xkcd comic:\nGit. Source: xkcd.com\nI particularly feel this way when wanting to contribute to others\u0026rsquo; open-source projects on GitHub. For this, we typically use the \u0026ldquo;fork-and-branch\u0026rdquo; workflow. I wanted to document my simple approach to this workflow here (for reference by my future self and others). The workflow comprises the following steps which are described in more detail in the subsequent sections:\n Fork a GitHub repository: navigate to a repository on GitHub and click the Fork button. Clone the repository locally: git clone https://github.com/user/repo.git. Add remote called \u0026ldquo;upstream\u0026rdquo; pointing to the original repository: git remote add upstream https://github.com/user/repo.git. Checkout a new branch (here called \u0026ldquo;new_feature\u0026rdquo;): git checkout -b new_feature Make desired changes to the local repository on this branch. Pull new changes from remote: git checkout master, git pull upstream master. Sync dev branch: git checkout new_feature, git merge master. Push changes to your remote repository: git push origin new_feature. Open a pull request on GitHub merging your changes with the upstream (original) repository. Once the pull request is accepted, you\u0026rsquo;ll want to pull those changes into your origin (forked repository). Change to master: git checkout master and pull: git pull upstream master. Delete your feature branch using the GitHub website or, delete the local branch: git branch -d new_feature, and delete the remote: git push origin --delete new_feature.  1. Forking a GitHub Repository The first step is to fork the GitHub repository you want to work on. A \u0026ldquo;fork\u0026rdquo; is just an independent copy of a repository that you can develop on without affecting the original. To fork a repository, find it on GitHub and then click the Fork button.\n2. Clone the repository locally Before you can make changes to the repository you\u0026rsquo;ll first want to make a local copy on your computer. This is as simple as using git clone on the forked repository. Navigate to your forked repository on GitHub, click the \u0026ldquo;Clone or download\u0026rdquo; button and copy the url. Then, at the command line, clone the repository, for example:\n$ git clone https://github.com/user/repo.git  3. Add a remote When you cloned the forked repository onto your local computer, git automatically added a remote repository named \u0026ldquo;origin\u0026rdquo; pointing to the forked repository on GitHub. This means that when you do git add/git commit/git push you can push your local changes to the forked repository.\nHowever, the goal here is to contribute to the original repository and we want to keep up to date with the original. While we are making changes, others might also be making changes and the original repository might be getting updated during the time you are adding a feature. So we want to add another remote pointing to the original repository so that we can periodically git pull any changes that have occurred in that repository such that we are working on the must up-to-date version of the code. We usually call this remote \u0026ldquo;upstream\u0026rdquo; and can add it using:\n$ git remote add upstream https://github.com/user/repo.git  You can verify that you now have two remotes, \u0026ldquo;origin\u0026rdquo; and \u0026ldquo;upstream\u0026rdquo; using the following:\n$ git remote -v  4. Checkout a new branch Okay, so now we\u0026rsquo;ve made a fork of the repository we want to work on, we\u0026rsquo;ve cloned it to our local computer and also added a remote pointing back to the original repository. Now we can start making our desired changes. To do this, we are going to want to create a branch to work on. This branch will be independent of the clean, functioning \u0026ldquo;master\u0026rdquo; code and is a safe place for you to delete, modify and add code. You can actually have multiple branches (for different features) that you\u0026rsquo;re working on at the same time. To create a branch called \u0026ldquo;new_feature\u0026rdquo;, use the following:\n$ git checkout -b new_feature  You can verify that you created the branch by using the following command which will show you all your local and remote branches:\n$ git branch -a  5. Make changes Now that you have an independent workspace (a branch) to work on, that will not break any of the existing code, you can get to work implementing your changes. As you work you will add and commit changes you make. It is likely that the longer you take to implement your changes, the more changes could be made to the original \u0026ldquo;upstream\u0026rdquo; code - which could be problematic, especially if the code you are changing on your branch also gets changed in the upstream repository, which can leave you with a bunch of troublesome \u0026ldquo;merge conflicts\u0026rdquo; to deal with later on. If you want to make sure your branch stays up-to-date with the original repository you forked, you\u0026rsquo;ll need to do two things. Firstly, update your \u0026ldquo;master\u0026rdquo; fork of the original repository by checking out the master branch and pulling from the upstream repository:\n$ git checkout master $ git pull upstream master  Then, go back to your branch and merge it with the master to incorporate any new changes:\n$ git checkout new_feature $ git merge master  6. Push changes When you\u0026rsquo;ve made all the changes you want to make to the code you\u0026rsquo;ll want to push it to your remote \u0026ldquo;origin\u0026rdquo; to get it ready to show to the maintainers of the \u0026ldquo;upstream\u0026rdquo; (original) repository you wanted to contribute to in the first place. To do this:\n$ git push origin new_feature  This will push your branch to your forked copy of the original repository.\n7. Open a pull request Finally, we can open a \u0026ldquo;pull request\u0026rdquo; which essentially asks the maintainers of the original repository to take a look at and hopefully integrate your code changes into their repository. To open a pull request, go to the GitHub website, navigate to your \u0026ldquo;new_feature\u0026rdquo; branch and follow the prompts to open a pull request. Note that for many popular repositories, there will be a number of tasks you should complete before opening a pull request. For example, checking that the code still passes a number of pre-written tests, that the documentation still renders, etc. These kinds of guidelines for contributing to a repository are typically included in a repository\u0026rsquo;s root directory in a file called something like \u0026ldquo;contributing.md\u0026rdquo;.\n8. Updating your fork Once the maintainers accept your changes, the code you wrote will now be incorporated into the original repository. Hooray! Once this is done, you\u0026rsquo;ll want to update your fork of the original repository (because it now includes the changes you added through your branch and pull request workflow). Locally, we will make sure we are on the master branch, and we will pull changes from the upstream (original) repository:\n$ git checkout master $ git pull upstream master  9. Deleting branches Finally we can now delete our feature branch (both locally and remotely) because we no longer need it:\n$ git branch -d new_feature $ git push origin --delete new_feature  Note that during the pull request workflow on GitHub, you may have already deleted your feature branch by following prompts on GitHub which is fine too.\n","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"82a412cc1920bc672ed4d126529ea6a3","permalink":"https://www.tomasbeuzen.com/post/git-fork-branch-pull/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/post/git-fork-branch-pull/","section":"post","summary":"A simple summary of how to collaborate on open-source projects using forking, branching and pull requests.","tags":[],"title":"The Git Fork-Branch-Pull Workflow","type":"post"},{"authors":["Tomas Beuzen"],"categories":[],"content":" Introduction I recently decided I wanted to create this personal website to serve as my online bio and to display various research and personal projects I am/have worked on. To my surprise, this actually turned out to be much easier than anticipated! So in this brief post I\u0026rsquo;ll walk you through how I created this website (for free!) in 5 simple steps using the static site generator Hugo and the static site hosting service provided by GitHub Pages.\n Install Hugo and get a GitHub account Set up a workspace Choose a theme for your website Creating your website Hosting your website on GitHub Pages Updating your website  Step 1: Install Hugo and get a GitHub account  As I\u0026rsquo;m a mac user, I\u0026rsquo;ll provide instructions here for installing Hugo on macOS. For instructions on installing Hugo on Windows or Linux, go to the Hugo Installation Page and then skip to Step 2. To install Hugo on macOS, you\u0026rsquo;ll ned both Homebrew and Go. Homebrew is a free and open-source software package management system that simplifies the installation of software on macOS and Go is a programming language created by Google that Hugo will leverage to create your website framework. If you don\u0026rsquo;t have Homebrew or Go installed on your computer, run the following in the terminal:\n$ /usr/bin/ruby -e \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026quot; $ brew install go  Now, to install Hugo simply type:\n$ brew install hugo  Finally, if you don\u0026rsquo;t already have a GitHub account, head over to the GitHub website and create one.\nStep 2: Set up a workspace  The next thing we need to do is create a directory from which we will build our website, this can be anywhere you like on your computer. You can create a directory using the macOS Finder or Windows File Explorer GUIs, or alternatively by using the command line. For example, the code below creates a new directory called websites in the Documents folder:\n$ cd ~/Documents $ mkdir websites  Now, from the command line, change into your new directory:\n$ cd websites  In this directory we will use Hugo to create the framework for your website. Simply type the following with \u0026ldquo;websitename\u0026rdquo; replaced with whatever you want to call your website:\n$ hugo new site websitename  After executing this command, you will see something like the following:\nCongratulations! Your new Hugo site is created in ~/Documents/websitename. Just a few more steps and you are ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026quot;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026quot; command. 2. Perhaps you want to add some content. You can add single files with \u0026quot;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026quot;. 3. Start the built-in live server via \u0026quot;hugo server\u0026quot;. Visit https://gohugo.io/ for quickstart guide and full documentation.  Change into your newly created websitename directory and view the contents using ls:\n$ cd websitename $ ls archetypes\tcontent\tlayouts\tthemes config.toml\tdata\tstatic  These files and folders form the framework of your Hugo site. We\u0026rsquo;ll explore them a little more later but for now, notice the themes folder. In the next step we will select a theme for your website and place it in this folder.\nStep 3: Choose a theme for your website!  With Hugo you can create your very own website theme or download one of many different pre-made, open source themes. There are plenty of really great pre-made themes and they are perfect for getting your website up and running as quickly as possible. Head over to the Hugo themes page and select a theme for your website. I used the Academic theme for my website.\nThe gallery of themes available to build a website with Hugo.\nOnce you\u0026rsquo;ve found a theme you like, click the download button which will take you to the theme\u0026rsquo;s GitHub repository. In the repository click the green Clone or download button and copy the https web URL. Go back to your terminal, cd to the themes directory and clone the repository:\n$ cd themes $ git clone https://github.com/gcushen/hugo-academic.git archetypes\tcontent\tlayouts\tthemes config.toml\tdata\tstatic  You can type ls to see that you now have a directory called hugo-academic (or whatever other theme you chose). Now cd into that directory and again type ls to see the content of the directory; these are the files and folders you will need to create your website.\nStep 4: Creating your website  I found that the quickest (and dirtiest) way to get your website up and running is to simply copy everything in the folder exampleSite to your directory websitename - overwrite any duplications. Once you\u0026rsquo;ve done that, go ahead and copy all the other folders and files in hugo-acaemic (except for the exampleSite folder, theme.toml, README.md and LICENSE.md) to your directory websitename - again, overwrite any duplications.\nExample of what your directory might look like before and after copying the necessary files.\nNow you can see what your website currently looks like by changing to the directory websitename and typing:\n$ hugo server  This renders a local version of your website so you can see it and make changes before putting it online. Copy and paste the url output (e.g., http://localhost:1313) into your browser of choice to see your website.\nAt this point go ahead and spend some time customizing your website. I recommend reading the documentation of the theme you chose to help navigate the folder and create new content. For example, here is the documentation for the academic theme. As you make changes, you should see that your website updates automatically in your browser - if for some reason it doesn\u0026rsquo;t, simply ctrl+c in your terminal, re-execute hugo server, and refresh your browser.\nStep 5: Hosting your website on GitHub Pages  Once you\u0026rsquo;re ready to show your shiny new website to the world, we can host it on GitHub Pages.\nThe first thing you need to do is go to GitHub and create two empty repositories:\n a repository with the same name as the website directory you created: \u0026lsquo;websitename\u0026rsquo;; and, a repository with the name \u0026lsquo;yourgithubusername.github.io\u0026rsquo;.  If you haven\u0026rsquo;t created a repository before, simply click your GitHub profile at the top right-hand corner of the screen to see a drop-down menu, click Your repositories and on that page you will se a green button New which will help you create a repository. Once you\u0026rsquo;ve done that, copy the https key from the repository you created with the same name as your website and then in the terminal cd to your local directory websitename and type:\n$ git init $ git remote add origin https://github.com/User/websitename.git $ git push -u origin master  This will create a git directory, link it to the remote GitHub repository and then push the contents of the local directory to the remote. Note that the -u argument specifies the master branch of the remote as \u0026ldquo;upstream\u0026rdquo;, meaning that in the future, a simple git push will suffice when pushing changes to the remote.\nNow copy the https key from the repository you created with the name yourgithubusername.github.io and copy the https key. Then cd back to your initial websites directory and type:\n$ git clone https://github.com/User/yourgithubusername.github.io.git # Clone the remote repo $ cd websitename # Change directory $ hugo -d ../yourgithubusername.github.io # Deploy your website $ cd ../yourgithubusername.github.io $ git add . $ git commit -m \u0026quot;first commit\u0026quot; $ git push -u origin master  And that\u0026rsquo;s it! Go to GitHub and go to the repository yourgithubusername.github.io, click the settings tab near the top of the page and scroll down to the subheading GitHub Pages where you will see the link to your new website: http://yourgitubusername.github.io. Click the link to see your website out in the wild. Congratulations - you can now show off your work to the world (wide web)!\nStep 6: Updating your website  If you are going to have content on your website that will change over time (e.g., writing posts, updating research projects, changing your job title, etc.) then you\u0026rsquo;ll need to know how to make these changes to your website. Luckily, this is as easy as pie. Make any changes/additions to your website in your local directory websitename (remember to use hugo server to render a local version of your website that you can view before deploying it online). Once you\u0026rsquo;ve made your changes make sure you\u0026rsquo;re in the websitename directory and type the following commands in terminal to push your local changes to the remote, re-deploy your website yourgithubusername.github.io and then push that to the remote too:\n$ git add . $ git commit -m \u0026quot;some changes\u0026quot; $ git push # push all changes to the remote $ hugo -d ../yourgithubusername.github.io # re-deploy website $ cd ../yourgithubusername.github.io $ git add . $ git commit -m \u0026quot;some changes\u0026quot; $ git push  After this, you may need to wait ~10 minutes for the changes to take effect on the online version of your website.\n","date":1566691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566691200,"objectID":"25f5178acb52657faa5ee13b82db6227","permalink":"https://www.tomasbeuzen.com/post/making-a-website-with-hugo/","publishdate":"2019-08-25T00:00:00Z","relpermalink":"/post/making-a-website-with-hugo/","section":"post","summary":"Learn how to quickly and easily build a website like this using Hugo in just 5 steps.","tags":[],"title":"Building your own website with Hugo and GitHub Pages","type":"post"},{"authors":null,"categories":null,"content":"","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"0e732fa7495949ce1391f33e8cd08906","permalink":"https://www.tomasbeuzen.com/project/calocrunch/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/project/calocrunch/","section":"project","summary":"Designed and developed the web application calocrunch.com.","tags":["Data Science","Graphics"],"title":"Calorie Calculator Web App","type":"project"},{"authors":null,"categories":null,"content":"","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"75c31109f69d7fb3ef5d2cf79849ccbe","permalink":"https://www.tomasbeuzen.com/project/deploy-ml/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/project/deploy-ml/","section":"project","summary":"Tutorials for deploying machine learning models with SageMaker and Flask.","tags":["Data Science","Machine Learning","Outreach"],"title":"Machine Learning Model Deployment Tutorials","type":"project"},{"authors":["Tomas Beuzen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"b0b2fed14ca53ef3ffb650dc90e17223","permalink":"https://www.tomasbeuzen.com/publication/journal-article-pybeach/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/journal-article-pybeach/","section":"publication","summary":"Sandy coastlines typically comprise two key parts; a beach and dune. The beach is the section of sandy coast that is mostly above water (depending upon tide) and actively influenced by waves, while dunes are elevated mounds/ridges of sand at the back of the beach. The interface between the beach and dune is often characterised by a distinct change in ground slope (with the dune having a steeper slope than the beach). Dunes are particularly important along sandy coastlines because they provide a natural barrier to coastal hazards such as storminduced waves and surge. The capacity of sandy dunes to provide coastal hazard protection depends in large part on their geometry. In particular, the location of the dune toe (the transition point between the beach and dune) is a key factor used in coastal erosion models and for assessing coastal vulnerability to hazards. pybeach is an open-source Python package that allows a user to quickly and effectively identify the dune toe location on 2D beach profiles.","tags":["Machine learning","data","dune","dune toe","coastal","runup","beach"],"title":"pybeach: A Python package for extracting the location of dune toes on beach profile transects","type":"publication"},{"authors":null,"categories":null,"content":"","date":1548547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548547200,"objectID":"74af12acd1b27bae7e3d2898419924b4","permalink":"https://www.tomasbeuzen.com/project/py-pkgs/","publishdate":"2019-01-27T00:00:00Z","relpermalink":"/project/py-pkgs/","section":"project","summary":"Co-authored open-source book about packaging and distributing Python code.","tags":["Data Science","Outreach"],"title":"Python Packaging Book","type":"project"},{"authors":["Tomas Beuzen","Joshua Simmons"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a5cea008173fe419e781e629328cb2ce","permalink":"https://www.tomasbeuzen.com/publication/journal-article-cvnvs/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/journal-article-cvnvs/","section":"publication","summary":"Bayesian Networks (BNs) are useful methods of probabilistically modelling environmental systems. BN performance is sensitive to the number of variables included in the model framework. The selection of the optimum set of variables to include in a BN (‚Äúvariable selection‚Äù) is therefore a key part of the BN modelling process. While variable selection is an issue dealt with in the wider BN and machine learning literature, it remains largely absent from environmental BN applications to date, due in large part to a lack of software designed to work with available BN packages. CVNetica_VS is an open-source Python module that extends the functionality of Netica, a commonly used commercial BN software package, to perform variable selection. CVNetica_VS uses wrapper-based variable selection and cross-validation to search for the optimum variable set to use in a BN. The software will aid in objectifying and automating the development of BNs in environmental applications.","tags":["Bayesian network","variable selection","overfitting"],"title":"A variable selection package driving Netica with Python","type":"publication"},{"authors":["Tomas Beuzen","Evan Goldstein","Kristen Splinter"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"91cfa54e41ec09e6f27264cc908c2b3f","permalink":"https://www.tomasbeuzen.com/publication/journal-article-gp/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/journal-article-gp/","section":"publication","summary":"After decades of study and significant data collection of time-varying swash on sandy beaches, there is no single deterministic prediction scheme for wave runup that eliminates prediction error ‚Äì even bespoke, locally tuned predictors present scatter when compared to observations. Scatter in runup prediction is meaningful and can be used to create probabilistic predictions of runup for a given wave climate and beach slope. This contribution demonstrates this using a data-driven Gaussian process predictor; a probabilistic machine-learning technique. The runup predictor is developed using 1 year of hourly wave runup data (8328 observations) collected by a fixed lidar at Narrabeen Beach, Sydney, Australia. The Gaussian process predictor accurately predicts hourly wave runup elevation when tested on unseen data with a root-mean-squared error of 0.18‚Äâm and bias of 0.02‚Äâm. The uncertainty estimates output from the probabilistic GP predictor are then used practically in a deterministic numerical model of coastal dune erosion, which relies on a parameterization of wave runup, to generate ensemble predictions. When applied to a dataset of dune erosion caused by a storm event that impacted Narrabeen Beach in 2011, the ensemble approach reproduced ‚àº85‚Äâ% of the observed variability in dune erosion along the 3.5 km beach and provided clear uncertainty estimates around these predictions. This work demonstrates how data-driven methods can be used with traditional deterministic models to develop ensemble predictions that provide more information and greater forecasting skill when compared to a single model using a deterministic parameterization ‚Äì an idea that could be applied more generally to other numerical models of geomorphic systems.","tags":["Machine learning","Gaussian process","data","runup","erosion"],"title":"Ensemble models from machine learning: an example of wave runup and coastal dune erosion","type":"publication"},{"authors":["Tomas Beuzen","Lucy Marshall","Kristen Splinter"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"c47f8f58ee3e53c012606f46ae507953","permalink":"https://www.tomasbeuzen.com/publication/journal-article-dis/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/journal-article-dis/","section":"publication","summary":"Bayesian Networks (BNs) are an increasingly popular method for modelling environmental systems. The discretization of continuous variables is often required to use BNs. There are three main methods of discretization; manual, unsupervised, and supervised. Here, we compare and demonstrate each approach with a BN that predicts coastal erosion. Results reveal that supervised discretization methods produced BNs of the highest average predictive skill (73.8%), followed by manual discretization (69.0%) and unsupervised discretization (64.8%). However, each method has specific advantages that may make them more suitable for particular applications. Manual methods can produce physical meaningful BNs, which is favorable in environmental modelling. Supervised methods can autonomously and optimally discretize variables and may be preferred when predictive skill is a modelling priority. Unsupervised methods are computationally simple and versatile. The optimal discretization scheme should consider both the performance and practicality of the scheme.","tags":["Bayesian network","discretization","model fitting"],"title":"A comparison of methods for discretizing continuous variables in Bayesian Networks","type":"publication"},{"authors":["Tomas Beuzen","Kristen Splinter","Lucy Marshall","Ian Turner","Mitchell Harley","Margaret Palmsten"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"624c4e50113e6ad7fb844427bfb6c881","permalink":"https://www.tomasbeuzen.com/publication/journal-article-bn/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/journal-article-bn/","section":"publication","summary":"Bayesian networks (BNs) are increasingly being used to model complex coastal processes due to their ability to integrate non-linear systems, their transparent probabilistic framework, and low computational cost. A BN may be suited to descriptive or predictive application. Descriptive BNs are highly calibrated models that are useful for better understanding the physics and causal relationships driving a system. Predictive BNs are generalisations of a system that have skill at predicting outside of the training domain. The predictive and descriptive usefulness of a BN depends on its complexity and the amount of data available to train it, but there is often a trade-off; higher descriptive skill comes at the cost of reduced predictive skill. To demonstrate the differences between predictive and descriptive BNs in a coastal engineering context, a BN to predict shoreline recession caused by coastal storm events is developed and tested using an extensive 10-year dataset incorporating 137 individual storms events monitored at Narrabeen-Collaroy Beach, Australia. A parsimonious approach to BN development is used to separately determine the optimum predictive and descriptive BNs for this dataset. Results show that for this dataset two quite different BNs can be developed; one that is optimized to achieve the highest predictive skill, and a second network that is optimized to maximize descriptive skill. The optimum predictive BN is found to comprise 3 nodes (variables) and can predict the shoreline recession caused by unseen storm events with a skill of 65%. The optimum descriptive BN is composed of 5 nodes and can reproduce 88% of the training dataset, but with more limited predictive capabilities. The uses and limitations of these two different approaches to BN formulation are illustrated with example applications to coastal process modelling. It is anticipated that the insights provided in this paper will help to clarify the further development of Bayesian Networks applied to coastal modelling.","tags":["Bayesian network","storm erosion","coastal processes","coastal modelling"],"title":"Bayesian Networks in coastal engineering: Distinguishing descriptive and predictive applications","type":"publication"},{"authors":null,"categories":null,"content":"","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"2a883ddcaf6b76a2c089e990624e42a1","permalink":"https://www.tomasbeuzen.com/project/bait-509/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/bait-509/","section":"project","summary":"Designed and instructed BAIT509 at the University of British Columbia, available on GitHub.","tags":["Data Science","Machine Learning","Outreach"],"title":"Public Machine Learning Course","type":"project"},{"authors":["Alexander Atkinson","Tom Baldock","Florent Birrien","David Callaghan","Peter Nielsen","Tomas Beuzen","Ian Turner","Chris Blenkinsopp","Roshanka Ranasinghe"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"78afb0968ac2f8bc68e3a120be8c1bac","permalink":"https://www.tomasbeuzen.com/publication/journal-article-bruun2/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/journal-article-bruun2/","section":"publication","summary":"Rising sea levels are expected to cause widespread coastal recession over the course of the next century. In this work, new insight into the response of sandy beaches to sea level rise is obtained through a series of compre- hensive experiments using monochromatic and random waves in medium scale laboratory wave flumes. Beach profile development from initially planar profiles, and a 2/3 power law profile, exposed to wave conditions that formed barred or bermed profiles and subsequent profile evolution following rises in water level and the same wave conditions are presented. Experiments assess profile response to a step-change in water level as well as the influence of sediment deposition above the still water level (e.g. overwash). A continuity based profile translation model (PTM) is applied to both idealised and measured shoreface profiles, and is used to predict overwash and deposition volumes above the shoreline. Quantitative agreement with the Bruun Rule (and variants of it) is found for measured shoreline recession for both barred and bermed beach profiles. There is some variability between the profiles at equilibrium at the two different water levels. Under these idealised conditions, deviations between the original Bruun Rule, the modification by Rosati et al. (2013) and the PTM model predictions are of the order of 15% and all these model predictions are within 30% of the observed shoreline recession. Measurements of the recession of individual contour responses, such as the shoreline, may be subject to local profile variability; therefore, a measure of the mean recession of the profile is also obtained by averaging the recession of discrete contours throughout the active profile. The mean recession only requires conservation of volume, not conser- vation of profile shape, to be consistent with the Bruun Rule concept, and is found to be in better agreement with all three model predictions than the recession measured at the shoreline.","tags":["sea level rise","coastal erosion","Bruun rule","coastal management","sediment transport"],"title":"Laboratory investigation of the Bruun Rule and beach response to sea level rise","type":"publication"},{"authors":["Tomas Beuzen","Ian Turner","Chris Blenkinsopp","Alex Atkinson","Francois Flocard","Tom Baldock"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"5cc570877ab5f44b4f043a630eaa9054","permalink":"https://www.tomasbeuzen.com/publication/journal-article-bruun/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/journal-article-bruun/","section":"publication","summary":"Persistent and accelerating sea level rise (SLR) may have a significant impact on the evolution of sandy coastlines this Century. The response of natural sandy beaches to SLR has been much discussed in the literature, however there is a lack of knowledge about the impact of SLR on engineered coasts. Laboratory experiments comprising over 320 h of testing were conducted in a 44 m (L) x 1.2 m (W) x 1.6 m (D) wave flume to investigate the in- fluence of coastal armouring in the form of seawalls on coastal response to SLR. The study was designed to investigate the effects of contrasting types of seawalls (reflective-impermeable versus dissipative-permeable) on beach profile response to increased water levels, in the presence of both erosive and accretionary wave conditions. The results obtained showed that seawalls alter the evolution of the equilibrium profile with rising water level, causing increased lowering of the profile adjacent to the structure. Under erosive wave conditions, modelled profiles both with and without seawall structures in place were observed to translate landward in response to SLR and erode the upper profile. It was found that the erosion demand at the upper beach due to a rise in water level remains similar whether a structure is present or not, but that a seawall concentrates the erosion in the area adjacent to the seawall, resulting in enhanced and localised profile lowering. The type of structure present (dissipative-permeable versus reflective-impermeable) was not observed to have a significant influence on this response. Under accretive conditions, the preservation of a large shoreface and berm resulted in no wave-structure interaction occurring, with the result that the presence of a seawall had no impact on profile evolution. A potential two-step method for estimating the observed profile response to water level rise in the presence of seawalls is proposed, whereby a simple profile translation model is used to provide a first estimate of the erosion demand, and then this eroded volume is redistributed in front of the seawall out to the position of the offshore bar.","tags":["sea level rise","seawalls","coastal erosion","Bruun rule","coastal management"],"title":"Physical model study of beach profile evolution by sea level rise in the presence of seawalls","type":"publication"},{"authors":["Tomas Beuzen","Chris Chickadel","Alexander Horner-Devine"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"882f93c805c08d51c738e62f99debe19","permalink":"https://www.tomasbeuzen.com/publication/journal-article-boils1/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/journal-article-boils1/","section":"publication","summary":"Thermal infrared (IR) imagery is combined with in situ flow measurements to examine the impact of subsurface stratification on boil activity in the tidally influenced Snohomish River. Boils at the river‚Äôs surface are an expression of bottom- generated turbulence, appearing as a disruption of the cool-skin surface layer in the IR imagery. Boil activity has previously been linked to the amount of aeration occurring in river systems. A synthesis of data across an ebb tide showed that when a tidal salinity intrusion retreated, turbulent kinetic energy and dissipation rapidly increased by 700% and 575%, respectively. Additionally, the mean boil area fraction in the IR field of view increased by almost 500% across the entire ebb tide time series, with approximately half of this change occurring during the period when the stratification ceased. Using an empirical method for estimating aeration, the change in areal fraction associated with the loss of density stratification is predicted to generate a more than 300% increase in the air-water gas flux.","tags":["aeration","rivers","stratification","thermal imaging","turbulence"],"title":"Physical model study of beach profile evolution by sea level rise in the presence of seawalls","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1fa4c0c2e392ce56e1f942a390a1f426","permalink":"https://www.tomasbeuzen.com/project/apple-watch-faces/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/apple-watch-faces/","section":"project","summary":"Custom-made Apple Watch faces with educational, sport, and science themes.","tags":["Graphics"],"title":"Custom Apple Watch faces","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"612b2d57b5595a0519b3e0d068079945","permalink":"https://www.tomasbeuzen.com/project/nrl-prediction/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/nrl-prediction/","section":"project","summary":"Data-driven model to predict the outcome of Australian National Rugby League (NRL) games.","tags":["Data Science","Prediction"],"title":"NRL game predictions","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2f807553c1faa83d89eab930c8bc66bd","permalink":"https://www.tomasbeuzen.com/project/runup-prediction/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/runup-prediction/","section":"project","summary":"A data-driven Gaussian process model of wave runup elevation on beaches","tags":["Machine Learning"],"title":"Predicting wave runup on beaches.","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"517c5159b1aec578b0411b94bc6a1697","permalink":"https://www.tomasbeuzen.com/project/unsw-jersey/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/unsw-jersey/","section":"project","summary":"Jersey designs created for the [University of New South Wales Water (UNSW) Research Centre](http://www.wrc.unsw.edu.au/).","tags":["Graphics"],"title":"UNSW Water Research Centre jersey designs.","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1175049229f801974a68adb06e49401e","permalink":"https://www.tomasbeuzen.com/project/unsw-wwd/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/unsw-wwd/","section":"project","summary":"Organiser of school outreach program for water quality education, [World Water Monitoring Day](http://www.worldwatermonitoringday.org/) and the [Streamwatch](https://www.streamwatch.org.au/) iniative.","tags":["Outreach"],"title":"World Water Monitoring Day outreach program.","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"dd1c1db021528748d65b9889c42143b2","permalink":"https://www.tomasbeuzen.com/project/autobeach/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/autobeach/","section":"project","summary":"A Python package for automatically identifying beach profile features such as the dune toe, dune crest and shoreline position","tags":["Machine Learning"],"title":"pybeach - a Python package for locating the dune toe on cross-shore beach profile transects.","type":"project"},{"authors":["Tomas Beuzen","Kristen Splinter","Ian Turner","Mitchell Harley","Lucy Marshall"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"01321bf7808212ad18e1386095db40bf","permalink":"https://www.tomasbeuzen.com/publication/conference-paper-cp1/","publishdate":"2015-06-01T00:00:00Z","relpermalink":"/publication/conference-paper-cp1/","section":"publication","summary":"Bayesian Networks (BNs) are increasingly being used to model coastal processes. BNs are probabilistic graphical models that are able to represent complex physical systems with the benefits of very low computational cost, intrinsic handling of uncertainty and error, and explicit description of causation and relationships between variables within the system. BNs can be used for both predictive and diagnostic inference, and are particularly suitable for application to management tools, as they are explicit in uncertainty, give outputs in probability distributions, and are relatively straight-forward to integrate within existing risk analysis and decision-making frameworks. The general steps in developing a Bayesian Network are described. An example application to predict the degree of shoreline erosion caused by coastal storms is then presented, based on a data set spanning 10 years that includes 137 individual storm events. Located at Collaroy-Narrabeen Beach in Sydney, Australia, an optimised BN is shown to correctly predict the extent of storm erosion (‚Äòextreme‚Äô, ‚Äòmild‚Äô, ‚Äòno-change‚Äô) 65% of the time when applied to unseen storm events. Sensitivity analysis shows that incident wave power and pre-storm shoreline position were the most sensitive parameters in the BN for predicting storm erosion at this site. The application of the BN to the June 2016 major ECL storm at Collaroy-Narrabeen shows how this simple approach could potentially be used in a real-world forecast application to inform emergency management decisions and assist with community preparedness. While possible, the development of a more complex BN with higher predictive skill requires more observation data than was available to this study, and is the topic of future work.","tags":["shoreline prediction","storms","shoreline erosion","Bayesian network"],"title":"Predicting storm erosion on sandy coastlines using a Bayesian network","type":"publication"}]