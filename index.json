[{"authors":["admin"],"categories":null,"content":"I'm a postdoctoral teaching and learning fellow for the Master of Data Science program at the University of British Columbia, Vancouver, Canada. I have experience in global ocean modelling, modelling coastal processes (particularly coastal storm erosion), coastal engineering, data science and machine learning. Currently, my primary interest is the application of machine learning and data science to Earth processes. I enjoy developing software and apps for workflow automation and would love to change the world üåè. ","date":1578614400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1578614400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://tomasbeuzen.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I'm a postdoctoral teaching and learning fellow for the Master of Data Science program at the University of British Columbia, Vancouver, Canada. I have experience in global ocean modelling, modelling coastal processes (particularly coastal storm erosion), coastal engineering, data science and machine learning. Currently, my primary interest is the application of machine learning and data science to Earth processes. I enjoy developing software and apps for workflow automation and would love to change the world üåè.","tags":null,"title":"Tomas Beuzen","type":"authors"},{"authors":["Tomas Beuzen"],"categories":[],"content":" Introduction Some of the key steps in a machine learning workflow are:\n feature preprocessing (encoding categorical features, scaling numeric features, transforming text data, etc.); feature selection (choosing which features to include in the model); model selection (choosing which machine learning estimator to use); and, hyperparameter tuning (determining the optimum hyperparameter values to use for each estimator).  It can be difficult to perform these tasks in an accurate, efficient and reproducible manner. In particular, it is important to ensure that, during cross-validation, feature preprocessing and feature selection are based only on the training portion of data, preventing leakage from the validation set which could bias our results. In this short, practical post I\u0026rsquo;ll demonstrate how to use scikit-learn to simultaneously perform the above steps. While the example given is somewhat contrived, the syntax and workflow are what is important here and can be applied to any machine learning workflow.\nStep 1: Import dependencies from sklearn.pipeline import Pipeline from sklearn.datasets import make_classification from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import SelectKBest, mutual_info_classif  Step 2: Import data We will create a synthetic binary classification dataset for this demonstration using the scikit-learn function make_classification.\nX, y = make_classification(n_samples=1000, n_features=30, n_informative=5, n_redundant=5, n_classes=2, random_state=123)  Step 3: Create pipeline framework Using our synthetic dataset, we are going to set up a pipeline object that will:\n Standardize the data using StandardScaler; Select the k best features from the data using SelectKBest and the mutual information metric (where k is a hyperparameter that we will tune during the fitting process); and, Use an estimator to model the data, here we will be trying a LogisticRegression, RandomForestClassifier, and KNeighborsClassifier.  The syntax for creating this pipeline is shown below. To instantiate the Pipeline object I\u0026rsquo;ve used a k value in SelectKBest of 5 and I\u0026rsquo;ve input LogisticRegression as the estimator, but these are simply placeholders for now and they will be varied during the fitting stage.\npipe = Pipeline([('scaler', StandardScaler()), ('selector', SelectKBest(mutual_info_classif, k=5)), ('classifier', LogisticRegression())])  Step 4: Create search space The next step is to define the space of hyperparameters and estimators we want to search through. We do this in the form of a dictionary and we use double underscore notation (__) to refer to the hyperparameters of different steps in our pipeline. We will be trying out different values of k for the feature selector SelectKBest, as well as different hyperparameter values for each of our three estimators as shown below.\nsearch_space = [{'selector__k': [5, 10, 20, 30]}, {'classifier': [LogisticRegression(solver='lbfgs')], 'classifier__C': [0.01, 0.1, 1.0]}, {'classifier': [RandomForestClassifier(n_estimators=100)], 'classifier__max_depth': [5, 10, None]}, {'classifier': [KNeighborsClassifier()], 'classifier__n_neighbors': [3, 7, 11], 'classifier__weights': ['uniform', 'distance']}]  Step 5: Run the GridSearch This is where the magic happens. We will now pass our pipeline into GridSearchCV to test our search space (of feature preprocessing, feature selection, model selection, and hyperparameter tuning combinations) using 10-fold cross-validation.\nclf = GridSearchCV(pipe, search_space, cv=10, verbose=0) clf = clf.fit(X, y)  Step 6: Get the results We can access the best result of our search using the best_estimator_ attribute. For this particular case, the KNeighborsClassifier did the best, using n_neighbors=3 and weights='distance', along with the k=5 best features chosen by SelectKBest. This combination had a 10-fold cross-validation accuracy of 0.958.\nclf.best_estimator_ Pipeline(memory=None, steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('selector', SelectKBest(k=5, score_func=\u0026lt;function mutual_info_classif at 0x1a1f91db00\u0026gt;)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=3, p=2, weights='distance'))], verbose=False) clf.best_score_ 0.958  ","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"5178b27001514a7320a191d620398782","permalink":"http://tomasbeuzen.github.io/post/scikit-learn-gridsearch-pipelines/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/post/scikit-learn-gridsearch-pipelines/","section":"post","summary":"How to easily perform simultaneous feature preprocessing, feature selection, model selection, and hyperparameter tuning in just a few lines of code using Python and scikit-learn.","tags":[],"title":"Simultaneous feature preprocessing, feature selection, model selection, and hyperparameter tuning in scikit-learn with Pipeline and GridSearchCV","type":"post"},{"authors":["Tomas Beuzen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"ea346015f1be4352daae219ca2f0565a","permalink":"http://tomasbeuzen.github.io/publication/journal-article-jgr/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/journal-article-jgr/","section":"publication","summary":"The erosion impact of large coastal storm events typically occurs across broad (100s of km) sections of coastline and may include significant variability both alongshore and vertically between the berm and dunes. Identifying controls of variability in storm erosion is critical to understanding the response of coastlines to present and changing storminess. This contribution analyses immediate pre‚Äê and post‚Äêstorm Lidar data of over 1700 cross‚Äêshore profile transects, determined at every 100 m alongshore and spanning 400km of the southeast Australian coastline. This unique dataset allowed for a data‚Äêdriven Bayesian network analysis of the key relationships between the measured storm erosion response and a range of variables describing the antecedent morphology and hydrodynamic forcing at the coastline. It was found that while erosion of the dune and berm was observed to increase with increased exposure of the local profile to incident storm waves, additional erosion controls were found to be different for these two different sections of the beach. Erosion of the berm was specifically linked to the pre‚Äêstorm berm volume, with more accreted berms experiencing a greater proportion of erosion of the overall berm, regardless of variability in forcing conditions. In contrast, dune erosion was equally controlled by the exceedance of wave runup above the antecedent dune toe elevation and the width of the beach immediately fronting the dune, with wider beaches resulting in reduced dune erosion. The results of this large, data‚Äêdriven analysis provide important affirmation and insights into the primary controls of berm and dune storm erosion.","tags":["Bayesian network","data","beach","dune","storm","erosion"],"title":"Controls of variability in berm and dune storm erosion","type":"publication"},{"authors":["Tomas Beuzen"],"categories":[],"content":" Introduction I recently decided I wanted to create this personal website to serve as my online bio and to display various research and personal projects I am/have worked on. To my surprise, this actually turned out to be much easier than anticipated! So in this brief post I\u0026rsquo;ll walk you through how I created this website (for free!) in 5 simple steps using the static site generator Hugo and the static site hosting service provided by GitHub Pages.\nStep 1: Install Hugo and get a GitHub account As I\u0026rsquo;m a mac user, I\u0026rsquo;ll provide instructions here for installing Hugo on macOS. For instructions on installing Hugo on Windows or Linux, go to the Hugo Installation Page and then skip to Step 2. To install Hugo on macOS, you\u0026rsquo;ll ned both Homebrew and Go. Homebrew is a free and open-source software package management system that simplifies the installation of software on macOS and Go is a programming language created by Google that Hugo will leverage to create your website framework. If you don\u0026rsquo;t have Homebrew or Go installed on your computer, run the following in the terminal:\n$ /usr/bin/ruby -e \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026quot; $ brew install go  Now, to install Hugo simply type:\n$ brew install hugo  Finally, if you don\u0026rsquo;t already have a GitHub account, head over to the GitHub website and create one.\nStep 2: Set up a workspace The next thing we need to do is create a directory from which we will build our website, this can be anywhere you like on your computer. You can create a directory using the macOS Finder or Windows File Explorer GUIs, or alternatively by using the command line. For example, the code below creates a new directory called websites in the Documents folder:\n$ cd ~/Documents $ mkdir websites  Now, from the command line, change into your new directory:\n$ cd websites  In this directory we will use Hugo to create the framework for your website. Simply type the following with \u0026ldquo;websitename\u0026rdquo; replaced with whatever you want to call your website:\n$ hugo new site websitename  After executing this command, you will see something like the following:\nCongratulations! Your new Hugo site is created in ~/Documents/websitename. Just a few more steps and you are ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026quot;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026quot; command. 2. Perhaps you want to add some content. You can add single files with \u0026quot;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026quot;. 3. Start the built-in live server via \u0026quot;hugo server\u0026quot;. Visit https://gohugo.io/ for quickstart guide and full documentation.  Change into your newly created websitename directory and view the contents using ls:\n$ cd websitename $ ls archetypes\tcontent\tlayouts\tthemes config.toml\tdata\tstatic  These files and folders form the framework of your Hugo site. We\u0026rsquo;ll explore them a little more later but for now, notice the themes folder. In the next step we will select a theme for your website and place it in this folder.\nStep 3: Choose a theme for your website! With Hugo you can create your very own website theme or download one of many different pre-made, open source themes. There are plenty of really great pre-made themes and they are perfect for getting your website up and running as quickly as possible. Head over to the Hugo themes page and select a theme for your website. I used the Academic theme for my website.\nThe gallery of themes available to build a website with Hugo.\nOnce you\u0026rsquo;ve found a theme you like, click the download button which will take you to the theme\u0026rsquo;s GitHub repository. In the repository click the green Clone or download button and copy the https web URL. Go back to your terminal, cd to the themes directory and clone the repository:\n$ cd themes $ git clone https://github.com/gcushen/hugo-academic.git archetypes\tcontent\tlayouts\tthemes config.toml\tdata\tstatic  You can type ls to see that you now have a directory called hugo-academic (or whatever other theme you chose). Now cd into that directory and again type ls to see the content of the directory; these are the files and folders you will need to create your website.\nStep 4: Creating your website I found that the quickest (and dirtiest) way to get your website up and running is to simply copy everything in the folder exampleSite to your directory websitename - overwrite any duplications. Once you\u0026rsquo;ve done that, go ahead and copy all the other folders and files in hugo-acaemic (except for the exampleSite folder, theme.toml, README.md and LICENSE.md) to your directory websitename - again, overwrite any duplications.\nExample of what your directory might look like before and after copying the necessary files.\nNow you can see what your website currently looks like by changing to the directory websitename and typing:\n$ hugo server  This renders a local version of your website so you can see it and make changes before putting it online. Copy and paste the url output (e.g., http://localhost:1313) into your browser of choice to see your website.\nAt this point go ahead and spend some time customizing your website. I recommend reading the documentation of the theme you chose to help navigate the folder and create new content. For example, here is the documentation for the academic theme. As you make changes, you should see that your website updates automatically in your browser - if for some reason it doesn\u0026rsquo;t, simply ctrl+c in your terminal, re-execute hugo server, and refresh your browser.\nStep 5: Hosting your website on GitHub Pages Once you\u0026rsquo;re ready to show your shiny new website to the world, we can host it on GitHub Pages.\nThe first thing you need to do is go to GitHub and create two empty repositories:\n a repository with the same name as the website directory you created: \u0026lsquo;websitename\u0026rsquo;; and, a repository with the name \u0026lsquo;yourgithubusername.github.io\u0026rsquo;.  If you haven\u0026rsquo;t created a repository before, simply click your GitHub profile at the top right-hand corner of the screen to see a drop-down menu, click Your repositories and on that page you will se a green button New which will help you create a repository. Once you\u0026rsquo;ve done that, copy the https key from the repository you created with the same name as your website and then in the terminal cd to your local directory websitename and type:\n$ git init $ git remote add origin https://github.com/User/websitename.git $ git push -u origin master  This will create a git directory, link it to the remote GitHub repository and then push the contents of the local directory to the remote. Note that the -u argument specifies the master branch of the remote as \u0026ldquo;upstream\u0026rdquo;, meaning that in the future, a simple git push will suffice when pushing changes to the remote.\nNow copy the https key from the repository you created with the name yourgithubusername.github.io and copy the https key. Then cd back to your initial websites directory and type:\n$ git clone https://github.com/User/yourgithubusername.github.io.git # Clone the remote repo $ cd websitename # Change directory $ hugo -d ../yourgithubusername.github.io # Deploy your website $ cd ../yourgithubusername.github.io $ git add . $ git commit -m \u0026quot;first commit\u0026quot; $ git push -u origin master  And that\u0026rsquo;s it! Go to GitHub and go to the repository yourgithubusername.github.io, click the settings tab near the top of the page and scroll down to the subheading GitHub Pages where you will see the link to your new website: http://yourgitubusername.github.io. Click the link to see your website out in the wild. Congratulations - you can now show off your work to the world (wide web)!\nStep 5.1: Updating your website If you are going to have content on your website that will change over time (e.g., writing posts, updating research projects, changing your job title, etc.) then you\u0026rsquo;ll need to know how to make these changes to your website. Luckily, this is as easy as pie. Make any changes/additions to your website in your local directory websitename (remember to use hugo server to render a local version of your website that you can view before deploying it online). Once you\u0026rsquo;ve made your changes make sure you\u0026rsquo;re in the websitename directory and type the following commands in terminal to push your local changes to the remote, re-deploy your website yourgithubusername.github.io and then push that to the remote too:\n$ git add . $ git commit -m \u0026quot;some changes\u0026quot; $ git push # push all changes to the remote $ hugo -d ../yourgithubusername.github.io # re-deploy website $ cd ../yourgithubusername.github.io $ git add . $ git commit -m \u0026quot;some changes\u0026quot; $ git push  After this, you may need to wait ~10 minutes for the changes to take effect on the online version of your website.\n","date":1566691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566691200,"objectID":"25f5178acb52657faa5ee13b82db6227","permalink":"http://tomasbeuzen.github.io/post/making-a-website-with-hugo/","publishdate":"2019-08-25T00:00:00Z","relpermalink":"/post/making-a-website-with-hugo/","section":"post","summary":"Learn how to quickly and easily build a website like this using Hugo in just 5 steps.","tags":[],"title":"Building your own website with Hugo and GitHub Pages","type":"post"},{"authors":null,"categories":null,"content":"","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"0e732fa7495949ce1391f33e8cd08906","permalink":"http://tomasbeuzen.github.io/project/calocrunch/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/project/calocrunch/","section":"project","summary":"Designed and developed the web application calocrunch.com.","tags":["Data Science","Graphics"],"title":"Calorie Calculator Web App","type":"project"},{"authors":["Tomas Beuzen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"b0b2fed14ca53ef3ffb650dc90e17223","permalink":"http://tomasbeuzen.github.io/publication/journal-article-pybeach/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/journal-article-pybeach/","section":"publication","summary":"Sandy coastlines typically comprise two key parts; a beach and dune. The beach is the section of sandy coast that is mostly above water (depending upon tide) and actively influenced by waves, while dunes are elevated mounds/ridges of sand at the back of the beach. The interface between the beach and dune is often characterised by a distinct change in ground slope (with the dune having a steeper slope than the beach). Dunes are particularly important along sandy coastlines because they provide a natural barrier to coastal hazards such as storminduced waves and surge. The capacity of sandy dunes to provide coastal hazard protection depends in large part on their geometry. In particular, the location of the dune toe (the transition point between the beach and dune) is a key factor used in coastal erosion models and for assessing coastal vulnerability to hazards. pybeach is an open-source Python package that allows a user to quickly and effectively identify the dune toe location on 2D beach profiles.","tags":["Machine learning","data","dune","dune toe","coastal","runup","beach"],"title":"pybeach: A Python package for extracting the location of dune toes on beach profile transects","type":"publication"},{"authors":null,"categories":null,"content":"","date":1548547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548547200,"objectID":"2a883ddcaf6b76a2c089e990624e42a1","permalink":"http://tomasbeuzen.github.io/project/bait-509/","publishdate":"2019-01-27T00:00:00Z","relpermalink":"/project/bait-509/","section":"project","summary":"Designed and instructed BAIT509 at the University of British Columbia, available on GitHub.","tags":["Data Science","Machine Learning","Outreach"],"title":"Public Machine Learning Course","type":"project"},{"authors":["Tomas Beuzen","Joshua Simmons"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a5cea008173fe419e781e629328cb2ce","permalink":"http://tomasbeuzen.github.io/publication/journal-article-cvnvs/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/journal-article-cvnvs/","section":"publication","summary":"Bayesian Networks (BNs) are useful methods of probabilistically modelling environmental systems. BN performance is sensitive to the number of variables included in the model framework. The selection of the optimum set of variables to include in a BN (‚Äúvariable selection‚Äù) is therefore a key part of the BN modelling process. While variable selection is an issue dealt with in the wider BN and machine learning literature, it remains largely absent from environmental BN applications to date, due in large part to a lack of software designed to work with available BN packages. CVNetica_VS is an open-source Python module that extends the functionality of Netica, a commonly used commercial BN software package, to perform variable selection. CVNetica_VS uses wrapper-based variable selection and cross-validation to search for the optimum variable set to use in a BN. The software will aid in objectifying and automating the development of BNs in environmental applications.","tags":["Bayesian network","variable selection","overfitting"],"title":"A variable selection package driving Netica with Python","type":"publication"},{"authors":["Tomas Beuzen","Evan Goldstein","Kristen Splinter"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"91cfa54e41ec09e6f27264cc908c2b3f","permalink":"http://tomasbeuzen.github.io/publication/journal-article-gp/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/journal-article-gp/","section":"publication","summary":"After decades of study and significant data collection of time-varying swash on sandy beaches, there is no single deterministic prediction scheme for wave runup that eliminates prediction error ‚Äì even bespoke, locally tuned predictors present scatter when compared to observations. Scatter in runup prediction is meaningful and can be used to create probabilistic predictions of runup for a given wave climate and beach slope. This contribution demonstrates this using a data-driven Gaussian process predictor; a probabilistic machine-learning technique. The runup predictor is developed using 1 year of hourly wave runup data (8328 observations) collected by a fixed lidar at Narrabeen Beach, Sydney, Australia. The Gaussian process predictor accurately predicts hourly wave runup elevation when tested on unseen data with a root-mean-squared error of 0.18‚Äâm and bias of 0.02‚Äâm. The uncertainty estimates output from the probabilistic GP predictor are then used practically in a deterministic numerical model of coastal dune erosion, which relies on a parameterization of wave runup, to generate ensemble predictions. When applied to a dataset of dune erosion caused by a storm event that impacted Narrabeen Beach in 2011, the ensemble approach reproduced ‚àº85‚Äâ% of the observed variability in dune erosion along the 3.5 km beach and provided clear uncertainty estimates around these predictions. This work demonstrates how data-driven methods can be used with traditional deterministic models to develop ensemble predictions that provide more information and greater forecasting skill when compared to a single model using a deterministic parameterization ‚Äì an idea that could be applied more generally to other numerical models of geomorphic systems.","tags":["Machine learning","Gaussian process","data","runup","erosion"],"title":"Ensemble models from machine learning: an example of wave runup and coastal dune erosion","type":"publication"},{"authors":["Tomas Beuzen","Lucy Marshall","Kristen Splinter"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"c47f8f58ee3e53c012606f46ae507953","permalink":"http://tomasbeuzen.github.io/publication/journal-article-dis/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/journal-article-dis/","section":"publication","summary":"Bayesian Networks (BNs) are an increasingly popular method for modelling environmental systems. The discretization of continuous variables is often required to use BNs. There are three main methods of discretization; manual, unsupervised, and supervised. Here, we compare and demonstrate each approach with a BN that predicts coastal erosion. Results reveal that supervised discretization methods produced BNs of the highest average predictive skill (73.8%), followed by manual discretization (69.0%) and unsupervised discretization (64.8%). However, each method has specific advantages that may make them more suitable for particular applications. Manual methods can produce physical meaningful BNs, which is favorable in environmental modelling. Supervised methods can autonomously and optimally discretize variables and may be preferred when predictive skill is a modelling priority. Unsupervised methods are computationally simple and versatile. The optimal discretization scheme should consider both the performance and practicality of the scheme.","tags":["Bayesian network","discretization","model fitting"],"title":"A comparison of methods for discretizing continuous variables in Bayesian Networks","type":"publication"},{"authors":["Tomas Beuzen","Kristen Splinter","Lucy Marshall","Ian Turner","Mitchell Harley","Margaret Palmsten"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"624c4e50113e6ad7fb844427bfb6c881","permalink":"http://tomasbeuzen.github.io/publication/journal-article-bn/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/journal-article-bn/","section":"publication","summary":"Bayesian networks (BNs) are increasingly being used to model complex coastal processes due to their ability to integrate non-linear systems, their transparent probabilistic framework, and low computational cost. A BN may be suited to descriptive or predictive application. Descriptive BNs are highly calibrated models that are useful for better understanding the physics and causal relationships driving a system. Predictive BNs are generalisations of a system that have skill at predicting outside of the training domain. The predictive and descriptive usefulness of a BN depends on its complexity and the amount of data available to train it, but there is often a trade-off; higher descriptive skill comes at the cost of reduced predictive skill. To demonstrate the differences between predictive and descriptive BNs in a coastal engineering context, a BN to predict shoreline recession caused by coastal storm events is developed and tested using an extensive 10-year dataset incorporating 137 individual storms events monitored at Narrabeen-Collaroy Beach, Australia. A parsimonious approach to BN development is used to separately determine the optimum predictive and descriptive BNs for this dataset. Results show that for this dataset two quite different BNs can be developed; one that is optimized to achieve the highest predictive skill, and a second network that is optimized to maximize descriptive skill. The optimum predictive BN is found to comprise 3 nodes (variables) and can predict the shoreline recession caused by unseen storm events with a skill of 65%. The optimum descriptive BN is composed of 5 nodes and can reproduce 88% of the training dataset, but with more limited predictive capabilities. The uses and limitations of these two different approaches to BN formulation are illustrated with example applications to coastal process modelling. It is anticipated that the insights provided in this paper will help to clarify the further development of Bayesian Networks applied to coastal modelling.","tags":["Bayesian network","storm erosion","coastal processes","coastal modelling"],"title":"Bayesian Networks in coastal engineering: Distinguishing descriptive and predictive applications","type":"publication"},{"authors":["Alexander Atkinson","Tom Baldock","Florent Birrien","David Callaghan","Peter Nielsen","Tomas Beuzen","Ian Turner","Chris Blenkinsopp","Roshanka Ranasinghe"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"78afb0968ac2f8bc68e3a120be8c1bac","permalink":"http://tomasbeuzen.github.io/publication/journal-article-bruun2/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/journal-article-bruun2/","section":"publication","summary":"Rising sea levels are expected to cause widespread coastal recession over the course of the next century. In this work, new insight into the response of sandy beaches to sea level rise is obtained through a series of compre- hensive experiments using monochromatic and random waves in medium scale laboratory wave flumes. Beach profile development from initially planar profiles, and a 2/3 power law profile, exposed to wave conditions that formed barred or bermed profiles and subsequent profile evolution following rises in water level and the same wave conditions are presented. Experiments assess profile response to a step-change in water level as well as the influence of sediment deposition above the still water level (e.g. overwash). A continuity based profile translation model (PTM) is applied to both idealised and measured shoreface profiles, and is used to predict overwash and deposition volumes above the shoreline. Quantitative agreement with the Bruun Rule (and variants of it) is found for measured shoreline recession for both barred and bermed beach profiles. There is some variability between the profiles at equilibrium at the two different water levels. Under these idealised conditions, deviations between the original Bruun Rule, the modification by Rosati et al. (2013) and the PTM model predictions are of the order of 15% and all these model predictions are within 30% of the observed shoreline recession. Measurements of the recession of individual contour responses, such as the shoreline, may be subject to local profile variability; therefore, a measure of the mean recession of the profile is also obtained by averaging the recession of discrete contours throughout the active profile. The mean recession only requires conservation of volume, not conser- vation of profile shape, to be consistent with the Bruun Rule concept, and is found to be in better agreement with all three model predictions than the recession measured at the shoreline.","tags":["sea level rise","coastal erosion","Bruun rule","coastal management","sediment transport"],"title":"Laboratory investigation of the Bruun Rule and beach response to sea level rise","type":"publication"},{"authors":["Tomas Beuzen","Ian Turner","Chris Blenkinsopp","Alex Atkinson","Francois Flocard","Tom Baldock"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"5cc570877ab5f44b4f043a630eaa9054","permalink":"http://tomasbeuzen.github.io/publication/journal-article-bruun/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/journal-article-bruun/","section":"publication","summary":"Persistent and accelerating sea level rise (SLR) may have a significant impact on the evolution of sandy coastlines this Century. The response of natural sandy beaches to SLR has been much discussed in the literature, however there is a lack of knowledge about the impact of SLR on engineered coasts. Laboratory experiments comprising over 320 h of testing were conducted in a 44 m (L) x 1.2 m (W) x 1.6 m (D) wave flume to investigate the in- fluence of coastal armouring in the form of seawalls on coastal response to SLR. The study was designed to investigate the effects of contrasting types of seawalls (reflective-impermeable versus dissipative-permeable) on beach profile response to increased water levels, in the presence of both erosive and accretionary wave conditions. The results obtained showed that seawalls alter the evolution of the equilibrium profile with rising water level, causing increased lowering of the profile adjacent to the structure. Under erosive wave conditions, modelled profiles both with and without seawall structures in place were observed to translate landward in response to SLR and erode the upper profile. It was found that the erosion demand at the upper beach due to a rise in water level remains similar whether a structure is present or not, but that a seawall concentrates the erosion in the area adjacent to the seawall, resulting in enhanced and localised profile lowering. The type of structure present (dissipative-permeable versus reflective-impermeable) was not observed to have a significant influence on this response. Under accretive conditions, the preservation of a large shoreface and berm resulted in no wave-structure interaction occurring, with the result that the presence of a seawall had no impact on profile evolution. A potential two-step method for estimating the observed profile response to water level rise in the presence of seawalls is proposed, whereby a simple profile translation model is used to provide a first estimate of the erosion demand, and then this eroded volume is redistributed in front of the seawall out to the position of the offshore bar.","tags":["sea level rise","seawalls","coastal erosion","Bruun rule","coastal management"],"title":"Physical model study of beach profile evolution by sea level rise in the presence of seawalls","type":"publication"},{"authors":["Tomas Beuzen","Chris Chickadel","Alexander Horner-Devine"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   # Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"882f93c805c08d51c738e62f99debe19","permalink":"http://tomasbeuzen.github.io/publication/journal-article-boils1/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/journal-article-boils1/","section":"publication","summary":"Thermal infrared (IR) imagery is combined with in situ flow measurements to examine the impact of subsurface stratification on boil activity in the tidally influenced Snohomish River. Boils at the river‚Äôs surface are an expression of bottom- generated turbulence, appearing as a disruption of the cool-skin surface layer in the IR imagery. Boil activity has previously been linked to the amount of aeration occurring in river systems. A synthesis of data across an ebb tide showed that when a tidal salinity intrusion retreated, turbulent kinetic energy and dissipation rapidly increased by 700% and 575%, respectively. Additionally, the mean boil area fraction in the IR field of view increased by almost 500% across the entire ebb tide time series, with approximately half of this change occurring during the period when the stratification ceased. Using an empirical method for estimating aeration, the change in areal fraction associated with the loss of density stratification is predicted to generate a more than 300% increase in the air-water gas flux.","tags":["aeration","rivers","stratification","thermal imaging","turbulence"],"title":"Physical model study of beach profile evolution by sea level rise in the presence of seawalls","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1fa4c0c2e392ce56e1f942a390a1f426","permalink":"http://tomasbeuzen.github.io/project/apple-watch-faces/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/apple-watch-faces/","section":"project","summary":"Custom-made Apple Watch faces with educational, sport, and science themes.","tags":["Graphics"],"title":"Custom Apple Watch faces","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"612b2d57b5595a0519b3e0d068079945","permalink":"http://tomasbeuzen.github.io/project/nrl-prediction/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/nrl-prediction/","section":"project","summary":"Data-driven model to predict the outcome of Australian National Rugby League (NRL) games.","tags":["Data Science","Prediction"],"title":"NRL game predictions","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2f807553c1faa83d89eab930c8bc66bd","permalink":"http://tomasbeuzen.github.io/project/runup-prediction/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/runup-prediction/","section":"project","summary":"A data-driven Gaussian process model of wave runup elevation on beaches","tags":["Machine Learning"],"title":"Predicting wave runup on beaches.","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"517c5159b1aec578b0411b94bc6a1697","permalink":"http://tomasbeuzen.github.io/project/unsw-jersey/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/unsw-jersey/","section":"project","summary":"Jersey designs created for the [University of New South Wales Water (UNSW) Research Centre](http://www.wrc.unsw.edu.au/).","tags":["Graphics"],"title":"UNSW Water Research Centre jersey designs.","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1175049229f801974a68adb06e49401e","permalink":"http://tomasbeuzen.github.io/project/unsw-wwd/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/unsw-wwd/","section":"project","summary":"Organiser of school outreach program for water quality education, [World Water Monitoring Day](http://www.worldwatermonitoringday.org/) and the [Streamwatch](https://www.streamwatch.org.au/) iniative.","tags":["Outreach"],"title":"World Water Monitoring Day outreach program.","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"dd1c1db021528748d65b9889c42143b2","permalink":"http://tomasbeuzen.github.io/project/autobeach/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/autobeach/","section":"project","summary":"A Python package for automatically identifying beach profile features such as the dune toe, dune crest and shoreline position","tags":["Machine Learning"],"title":"pybeach - a Python package for locating the dune toe on cross-shore beach profile transects.","type":"project"},{"authors":["Tomas Beuzen","Kristen Splinter","Ian Turner","Mitchell Harley","Lucy Marshall"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"01321bf7808212ad18e1386095db40bf","permalink":"http://tomasbeuzen.github.io/publication/conference-paper-cp1/","publishdate":"2015-06-01T00:00:00Z","relpermalink":"/publication/conference-paper-cp1/","section":"publication","summary":"Bayesian Networks (BNs) are increasingly being used to model coastal processes. BNs are probabilistic graphical models that are able to represent complex physical systems with the benefits of very low computational cost, intrinsic handling of uncertainty and error, and explicit description of causation and relationships between variables within the system. BNs can be used for both predictive and diagnostic inference, and are particularly suitable for application to management tools, as they are explicit in uncertainty, give outputs in probability distributions, and are relatively straight-forward to integrate within existing risk analysis and decision-making frameworks. The general steps in developing a Bayesian Network are described. An example application to predict the degree of shoreline erosion caused by coastal storms is then presented, based on a data set spanning 10 years that includes 137 individual storm events. Located at Collaroy-Narrabeen Beach in Sydney, Australia, an optimised BN is shown to correctly predict the extent of storm erosion (‚Äòextreme‚Äô, ‚Äòmild‚Äô, ‚Äòno-change‚Äô) 65% of the time when applied to unseen storm events. Sensitivity analysis shows that incident wave power and pre-storm shoreline position were the most sensitive parameters in the BN for predicting storm erosion at this site. The application of the BN to the June 2016 major ECL storm at Collaroy-Narrabeen shows how this simple approach could potentially be used in a real-world forecast application to inform emergency management decisions and assist with community preparedness. While possible, the development of a more complex BN with higher predictive skill requires more observation data than was available to this study, and is the topic of future work.","tags":["shoreline prediction","storms","shoreline erosion","Bayesian network"],"title":"Predicting storm erosion on sandy coastlines using a Bayesian network","type":"publication"}]